{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project3_VariationalAutoencoder1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/channachot/Project1/blob/master/Project3_VariationalAutoencoder1.ipynb",
      "authorship_tag": "ABX9TyNqThJMAGxtn6amXbTemGej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/channachot/Project1/blob/master/Project3_VariationalAutoencoder1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAtBHnnkewO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXGF8iZtfphO",
        "colab_type": "code",
        "outputId": "cbfe89a9-a1eb-417e-f2ec-3f67eca49ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "WEIGHTS_FOLDER = 'weights'\n",
        "DATA_FOLDER = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "if not os.path.exists(WEIGHTS_FOLDER):\n",
        "  os.makedirs(os.path.join(WEIGHTS_FOLDER,\"AE\"))\n",
        "  os.makedirs(os.path.join(WEIGHTS_FOLDER,\"VAE\"))\n",
        "\n",
        "#Unzip the dataset downloaded from kaggle\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('/content/drive/My Drive/Colab Notebooks/animefacedataset.zip', 'r') as zipObj:\n",
        "    #Extract all the contents of zip file in the data directory\n",
        "    zipObj.extractall('/content/drive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-9f6f95a0de9e>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    with ZipFile('/content/drive/My Drive/Colab Notebooks/animefacedataset.zip, 'r') as zipObj:\u001b[0m\n\u001b[0m                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LTvOTKvfpkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from keras.utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAdYLqFGfpmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames = np.array(glob(os.path.join(DATA_FOLDER, '*/*.jpg')))\n",
        "NUM_IMAGES = len(filenames)\n",
        "print(\"Total number of images : \" + str(NUM_IMAGES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZrNTANWfpvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = (128,128,3) # Image dimension\n",
        "BATCH_SIZE = 512\n",
        "Z_DIM = 200 # Dimension of the latent vector (z)\n",
        "\n",
        "data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(DATA_FOLDER, \n",
        "                                                                   target_size = INPUT_DIM[:2],\n",
        "                                                                   batch_size = BATCH_SIZE,\n",
        "                                                                   shuffle = True,\n",
        "                                                                   class_mode = 'input',\n",
        "                                                                   subset = 'training'\n",
        "                                                                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svQBpmSwf_ht",
        "colab_type": "text"
      },
      "source": [
        "# Model VariationalAutoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRi-n-5QfpyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ENCODER\n",
        "def build_vae_encoder(input_dim, output_dim, conv_filters, conv_kernel_size, \n",
        "                  conv_strides, use_batch_norm = False, use_dropout = False):\n",
        "  \n",
        "  # Clear tensorflow session to reset layer index numbers to 0 for LeakyRelu, \n",
        "  # BatchNormalization and Dropout.\n",
        "  # Otherwise, the names of above mentioned layers in the model \n",
        "  # would be inconsistent\n",
        "  global K\n",
        "  K.clear_session()\n",
        "  \n",
        "  # Number of Conv layers\n",
        "  n_layers = len(conv_filters)\n",
        "\n",
        "  # Define model input\n",
        "  encoder_input = Input(shape = input_dim, name = 'encoder_input')\n",
        "  x = encoder_input\n",
        "\n",
        "  # Add convolutional layers\n",
        "  for i in range(n_layers):\n",
        "      x = Conv2D(filters = conv_filters[i], \n",
        "                  kernel_size = conv_kernel_size[i],\n",
        "                  strides = conv_strides[i], \n",
        "                  padding = 'same',\n",
        "                  name = 'encoder_conv_' + str(i)\n",
        "                  )(x)\n",
        "      if use_batch_norm:\n",
        "        x = BathcNormalization()(x)\n",
        "  \n",
        "      x = LeakyReLU()(x)\n",
        "\n",
        "      if use_dropout:\n",
        "        x = Dropout(rate=0.25)(x)\n",
        "\n",
        "  # Required for reshaping latent vector while building Decoder\n",
        "  shape_before_flattening = K.int_shape(x)[1:] \n",
        "  \n",
        "  x = Flatten()(x)\n",
        "  \n",
        "  mean_mu = Dense(output_dim, name = 'mu')(x)\n",
        "  log_var = Dense(output_dim, name = 'log_var')(x)\n",
        "\n",
        "  # Defining a function for sampling\n",
        "  def sampling(args):\n",
        "    mean_mu, log_var = args\n",
        "    epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n",
        "    return mean_mu + K.exp(log_var/2)*epsilon   \n",
        "  \n",
        "  # Using a Keras Lambda Layer to include the sampling function as a layer \n",
        "  # in the model\n",
        "  encoder_output = Lambda(sampling, name='encoder_output')([mean_mu, log_var])\n",
        "\n",
        "  return encoder_input, encoder_output, mean_mu, log_var, shape_before_flattening, Model(encoder_input, encoder_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU_jnL4-fp0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae_encoder_input, vae_encoder_output,  mean_mu, log_var, vae_shape_before_flattening, vae_encoder  = build_vae_encoder(input_dim = INPUT_DIM,\n",
        "                                    output_dim = Z_DIM, \n",
        "                                    conv_filters = [32, 64, 64, 64],\n",
        "                                    conv_kernel_size = [3,3,3,3],\n",
        "                                    conv_strides = [2,2,2,2])\n",
        "\n",
        "vae_encoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tRMpTLEBbVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder\n",
        "def build_decoder(input_dim, vae_shape_before_flattening, conv_filters, conv_kernel_size, \n",
        "                  conv_strides):\n",
        "\n",
        "  # Number of Conv layers\n",
        "  n_layers = len(conv_filters)\n",
        "\n",
        "  # Define model input\n",
        "  decoder_input = Input(shape = (input_dim,) , name = 'decoder_input')\n",
        "\n",
        "  # To get an exact mirror image of the encoder\n",
        "  x = Dense(np.prod(vae_shape_before_flattening))(decoder_input)\n",
        "  x = Reshape(vae_shape_before_flattening)(x)\n",
        "\n",
        "  # Add convolutional layers\n",
        "  for i in range(n_layers):\n",
        "      x = Conv2DTranspose(filters = conv_filters[i], \n",
        "                  kernel_size = conv_kernel_size[i],\n",
        "                  strides = conv_strides[i], \n",
        "                  padding = 'same',\n",
        "                  name = 'decoder_conv_' + str(i)\n",
        "                  )(x)\n",
        "      \n",
        "      # Adding a sigmoid layer at the end to restrict the outputs \n",
        "      # between 0 and 1\n",
        "      if i < n_layers - 1:\n",
        "        x = LeakyReLU()(x)\n",
        "      else:\n",
        "        x = Activation('sigmoid')(x)\n",
        "\n",
        "  # Define model output\n",
        "  decoder_output = x\n",
        "\n",
        "  return decoder_input, decoder_output, Model(decoder_input, decoder_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKGAE3MogHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae_decoder_input, vae_decoder_output, vae_decoder = build_decoder(input_dim = Z_DIM,\n",
        "                                        vae_shape_before_flattening = vae_shape_before_flattening,\n",
        "                                        conv_filters = [64,64,32,3],\n",
        "                                        conv_kernel_size = [3,3,3,3],\n",
        "                                        conv_strides = [2,2,2,2]\n",
        "                                        )\n",
        "vae_decoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMj4o9owgHg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The input to the model will be the image fed to the encoder.\n",
        "vae_input = vae_encoder_input\n",
        "\n",
        "# Output will be the output of the decoder. The term - decoder(encoder_output) \n",
        "# combines the model by passing the encoder output to the input of the decoder.\n",
        "vae_output = vae_decoder(vae_encoder_output)\n",
        "\n",
        "# Input to the combined model will be the input to the encoder.\n",
        "# Output of the combined model will be the output of the decoder.\n",
        "vae_model = Model(vae_input, vae_output)\n",
        "\n",
        "vae_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyslwOz8gHlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "N_EPOCHS = 200\n",
        "LOSS_FACTOR = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fggDpXE4gHn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def r_loss(y_true, y_pred):\n",
        "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
        "\n",
        "def kl_loss(y_true, y_pred):\n",
        "    kl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean_mu) - K.exp(log_var), axis = 1)\n",
        "    return kl_loss\n",
        "\n",
        "def total_loss(y_true, y_pred):\n",
        "    return LOSS_FACTOR*r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8TEqSugHjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam_optimizer = Adam(lr = LEARNING_RATE)\n",
        "\n",
        "vae_model.compile(optimizer=adam_optimizer, loss = total_loss, metrics = [r_loss, kl_loss])\n",
        "\n",
        "checkpoint_vae = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER, 'VAE/weights.h5'), save_weights_only = True, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFCp4muAgXam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae_model.fit_generator(data_flow, \n",
        "                        shuffle=True, \n",
        "                        epochs = N_EPOCHS, \n",
        "                        initial_epoch = 0, \n",
        "                        steps_per_epoch=NUM_IMAGES / BATCH_SIZE,\n",
        "                        callbacks=[checkpoint_vae])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWqKWyYJgXdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_batch = next(data_flow)\n",
        "example_batch = example_batch[0]\n",
        "example_images = example_batch[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNFL8L6NgXiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_compare_vae(images=None):\n",
        "  \n",
        "  if images is None:\n",
        "    example_batch = next(data_flow)\n",
        "    example_batch = example_batch[0]\n",
        "    images = example_batch[:10]\n",
        "\n",
        "  n_to_show = images.shape[0]\n",
        "  reconst_images = vae_model.predict(images)\n",
        "\n",
        "  fig = plt.figure(figsize=(15, 3))\n",
        "  fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "  for i in range(n_to_show):\n",
        "      img = images[i].squeeze()\n",
        "      sub = fig.add_subplot(2, n_to_show, i+1)\n",
        "      sub.axis('off')        \n",
        "      sub.imshow(img)\n",
        "\n",
        "  for i in range(n_to_show):\n",
        "      img = reconst_images[i].squeeze()\n",
        "      sub = fig.add_subplot(2, n_to_show, i+n_to_show+1)\n",
        "      sub.axis('off')\n",
        "      sub.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQlRPFrigXmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_compare_vae(images = example_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNDNKnl-gXkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vae_generate_images(n_to_show=10):\n",
        "  reconst_images = vae_decoder.predict(np.random.normal(0,1,size=(n_to_show,Z_DIM)))\n",
        "\n",
        "  fig = plt.figure(figsize=(15, 3))\n",
        "  fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "  for i in range(n_to_show):\n",
        "        img = reconst_images[i].squeeze()\n",
        "        sub = fig.add_subplot(2, n_to_show, i+1)\n",
        "        sub.axis('off')        \n",
        "        sub.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB1ubao4gXgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae_generate_images(n_to_show=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD6BxKNEgkz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fIKN06Ngk5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z_test = vae_encoder.predict(example_batch[:200])\n",
        "\n",
        "x = np.linspace(-3, 3, 300)\n",
        "\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "fig.subplots_adjust(hspace=0.6, wspace=0.4)\n",
        "\n",
        "for i in range(50):\n",
        "    ax = fig.add_subplot(5, 10, i+1)\n",
        "    ax.hist(z_test[:,i], density=True, bins = 20)\n",
        "    ax.axis('off')\n",
        "    ax.text(0.5, -0.35, str(i), fontsize=10, ha='center', transform=ax.transAxes)\n",
        "    ax.plot(x,norm.pdf(x))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIXsz19Zmmcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}